import torch
import numpy as np
from uni2ts.model.moirai.pretrain_patched import MoiraiPretrainPatched
from uni2ts.distribution import StudentTOutput

def test_pretrain_patched_init():
    """
    Test that MoiraiPretrainPatched initializes correctly and constructs the transform pipeline.
    """
    # Mock parameters
    module_kwargs = {
        "distr_output": StudentTOutput(),
        "d_model": 128,
        "num_layers": 2,
        "patch_sizes": [32, 64],
        "max_seq_len": 32,
        "attn_dropout_p": 0.0,
        "dropout_p": 0.0,
    }
    
    model = MoiraiPretrainPatched(
        min_patches=2,
        min_mask_ratio=0.1,
        max_mask_ratio=0.5,
        max_dim=4,
        num_training_steps=10,
        num_warmup_steps=0,
        module_kwargs=module_kwargs,
        enable_preconditioning=True,
        precondition_type="chebyshev",
        precondition_degree=2,
        learnable_preconditioning=False,
    )
    
    # Check transform map
    transform_map = model.train_transform_map
    default_transform = transform_map["default"]()
    
    # Verify PatchPolynomialPrecondition is in the pipeline
    # We can inspect the chain
    from uni2ts.transform.patch_precondition import PatchPolynomialPrecondition
    
    found = False
    # The transform is a Chain (Transformation + Transformation)
    # We need to traverse it.
    # Actually, uni2ts Transformation composition might not expose a simple list.
    # But we can check if it runs without error on dummy data.
    
    print("Model initialized successfully")
    return model

def test_pretrain_patched_transform():
    """
    Test that the transform pipeline runs on dummy data.
    """
    model = test_pretrain_patched_init()
    transform = model.train_transform_map["default"]()
    
    # Create dummy data entry
    # Needs fields expected by the pipeline
    data_entry = {
        "target": np.random.randn(1, 100).astype(np.float32), # (var, time)
        "freq": "H",
        "item_id": "item_0",
    }
    
    # Run transform
    # Note: Some transforms might require randomness or specific shapes
    # The pipeline includes:
    # SampleDimension -> GetPatchSize -> PatchCrop -> ... -> Patchify -> PatchPolynomialPrecondition -> ...
    
    # We need to handle potential randomness in GetPatchSize
    # But for testing we just want to see if it crashes
    
    try:
        transformed = transform(data_entry)
        print("Transform pipeline ran successfully")
        
        # Check if output has expected fields
        # Note: sample_id is NOT in the transform output - it's generated by the collator
        assert "target" in transformed
        assert "observed_mask" in transformed
        assert "time_id" in transformed
        assert "variate_id" in transformed
        assert "prediction_mask" in transformed
        assert "patch_size" in transformed
        
        # Check target shape: (combine_seq, patch_size)
        # Flattened by FlatPackFields
        target = transformed["target"]
        max_patch_size = max(model.module.patch_sizes)
        print(f"Target shape: {target.shape}, Max patch size: {max_patch_size}")
        assert target.shape[-1] == max_patch_size
        
        # Check if preconditioning metadata is present (if enabled)
        # Wait, FlatPackFields might not preserve metadata in the top level dict?
        # Metadata is added to data_entry.
        # But transform returns a new dict?
        # uni2ts Transformation usually modifies in place or returns modified.
        # FlatPackFields might create a new entry or modify.
        
        # Let's check if we can find evidence of preconditioning.
        # If PatchPolynomialPrecondition ran, it should have added "precondition_coeffs" to data_entry.
        # But SelectFields at the end filters keys.
        # So we might not see it in the final output.
        
        # However, if it didn't crash, it means shapes were compatible.
        
    except Exception as e:
        print(f"Transform failed: {e}")
        raise e

if __name__ == "__main__":
    test_pretrain_patched_transform()
