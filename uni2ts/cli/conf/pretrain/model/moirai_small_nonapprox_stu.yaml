# Non-Approx STU-MOIRAI Small Configuration
# Uses full M_phi_plus [K,d,d] spectral projection matrices instead of factorized approximation.
#
# With num_eigh=2 and dual-branch (use_hankel_L=false):
#   M_phi_plus [2, 384, 384] = 294,912 params
#   M_phi_minus [2, 384, 384] = 294,912 params
#   Total STU mixing: 589,824 ~ 589,964 (attention mixing)
#
# This gives the same per-layer parameter count as attention, so no FFN
# adjustment is needed. Clean apples-to-apples comparison.

_target_: uni2ts.model.moirai.pretrain_hybrid.MoiraiHybridPretrain
module_kwargs:
  _target_: builtins.dict
  distr_output:
    _target_: uni2ts.distribution.MixtureOutput
    components:
      - _target_: uni2ts.distribution.StudentTOutput
      - _target_: uni2ts.distribution.NormalFixedScaleOutput
      - _target_: uni2ts.distribution.NegativeBinomialOutput
      - _target_: uni2ts.distribution.LogNormalOutput
  d_model: 384
  num_layers: 6
  patch_sizes: ${as_tuple:[8, 16, 32, 64, 128]}
  max_seq_len: 512
  attn_dropout_p: 0.0
  dropout_p: 0.0
  scaling: true
  # Non-approx STU configuration
  stu_layer_pattern: alternating
  num_eigh: 2             # Only 2 spectral filters (non-approx has K*d*d params per branch)
  use_hankel_L: false     # Dual-branch: M_phi_plus + M_phi_minus
  use_approx: false       # KEY: use full M_phi matrices instead of factorized approximation
  use_nonapprox_stu: true
  use_variate_aware_stu: false
min_patches: 2
min_mask_ratio: 0.15
max_mask_ratio: 0.5
max_dim: 128
loss_func:
  _target_: uni2ts.loss.packed.PackedNLLLoss
lr: 1e-3
weight_decay: 1e-1
beta1: 0.9
beta2: 0.98
num_training_steps: ${mul:${trainer.max_epochs},${train_dataloader.num_batches_per_epoch}}
num_warmup_steps: 10_000
