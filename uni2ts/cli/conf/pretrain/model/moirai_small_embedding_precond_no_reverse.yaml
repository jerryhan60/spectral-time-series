# Moirai Small with Embedding-Level Preconditioning (No Reversal)
#
# Same as moirai_small_embedding_precond but WITHOUT the compensating filter
# after the transformer. Use this for comparison experiments.

defaults:
  - moirai_small_embedding_precond

# Override: disable reversal
module_kwargs:
  _target_: builtins.dict
  distr_output:
    _target_: uni2ts.distribution.MixtureOutput
    components:
      - _target_: uni2ts.distribution.StudentTOutput
      - _target_: uni2ts.distribution.NormalFixedScaleOutput
      - _target_: uni2ts.distribution.NegativeBinomialOutput
      - _target_: uni2ts.distribution.LogNormalOutput
  d_model: 384
  num_layers: 6
  patch_sizes: ${as_tuple:[8, 16, 32, 64, 128]}
  max_seq_len: 512
  attn_dropout_p: 0.0
  dropout_p: 0.0
  scaling: true
  # Embedding-level preconditioning: forward only, no reversal
  enable_embedding_preconditioning: true
  embedding_precondition_type: chebyshev
  embedding_precondition_degree: 5
  embedding_precondition_reverse: false  # No compensating filter
