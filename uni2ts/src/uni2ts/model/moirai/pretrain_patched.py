#  Copyright (c) 2024, Salesforce, Inc.
#  SPDX-License-Identifier: Apache-2
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

from collections import defaultdict
from collections.abc import Callable
from typing import Any

from uni2ts.transform import (
    AddObservedMask,
    AddSampleIndex,
    AddTimeIndex,
    AddVariateIndex,
    DefaultPatchSizeConstraints,
    DummyValueImputation,
    ExtendMask,
    FlatPackCollection,
    FlatPackFields,
    GetPatchSize,
    ImputeTimeSeries,
    MaskedPrediction,
    PackFields,
    PatchCrop,
    Patchify,
    SampleDimension,
    SelectFields,
    SequencifyField,
    Transformation,
)
from uni2ts.transform.patch_precondition import PatchPolynomialPrecondition
from .pretrain import MoiraiPretrain

class MoiraiPretrainPatched(MoiraiPretrain):
    """
    MoiraiPretrain with patch-level preconditioning.

    This class overrides the default training transform to apply preconditioning
    AFTER patching, instead of before.

    IMPORTANT: This class does NOT support learnable_preconditioning=True or
    loss_in_original_space=True because these features require time-step-level
    preconditioning logic that is incompatible with patch-level preconditioning.
    Use static preconditioning (learnable_preconditioning=False) and compute
    loss in preconditioned space (loss_in_original_space=False) when using this class.
    """
    # Note: We inherit seq_fields and pad_func_map from MoiraiPretrain.
    # sample_id is NOT in seq_fields because it is generated by the collator
    # (PackCollate), not by the transform pipeline.

    def __init__(self, *args, **kwargs):
        # Validate that incompatible options are not used
        learnable = kwargs.get('learnable_preconditioning', False)
        loss_orig = kwargs.get('loss_in_original_space', False)

        if learnable:
            raise ValueError(
                "MoiraiPretrainPatched does not support learnable_preconditioning=True. "
                "Learnable preconditioning operates at the time-step level, which is "
                "incompatible with patch-level preconditioning. Use MoiraiPretrain instead "
                "for learnable preconditioning, or set learnable_preconditioning=False."
            )

        if loss_orig:
            raise ValueError(
                "MoiraiPretrainPatched does not support loss_in_original_space=True. "
                "The reversal logic for loss computation uses time-step level reversal, "
                "which is incompatible with patch-level preconditioning. Use MoiraiPretrain "
                "instead, or set loss_in_original_space=False."
            )

        super().__init__(*args, **kwargs)
    
    @property
    def train_transform_map(self) -> dict[str, Callable[..., Transformation]]:
        """
        Get a dictionary of Transforms, with a default Transform as defined:
        ...
        Patchify: Perform patching
        PatchPolynomialPrecondition: Apply preconditioning to patches
        ...
        """

        def default_train_transform():
            # Start with standard transforms, but WITHOUT preconditioning at the start
            
            transform = (
                SampleDimension(
                    max_dim=self.hparams.max_dim,
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                )
                + GetPatchSize(
                    min_time_patches=self.hparams.min_patches,
                    target_field="target",
                    patch_sizes=self.module.patch_sizes,
                    patch_size_constraints=DefaultPatchSizeConstraints(),
                    offset=True,
                )
                + PatchCrop(
                    min_time_patches=self.hparams.min_patches,
                    max_patches=self.module.max_seq_len,
                    will_flatten=True,
                    offset=True,
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                )
                + PackFields(
                    output_field="target",
                    fields=("target",),
                    feat=False,
                )
                + PackFields(
                    output_field="past_feat_dynamic_real",
                    fields=tuple(),
                    optional_fields=("past_feat_dynamic_real",),
                    feat=False,
                )
                + AddObservedMask(
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    observed_mask_field="observed_mask",
                    collection_type=dict,
                )
                + ImputeTimeSeries(
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    imputation_method=DummyValueImputation(value=0.0),
                )
                + Patchify(
                    max_patch_size=max(self.module.patch_sizes),
                    fields=("target", "observed_mask"),
                    optional_fields=("past_feat_dynamic_real",),
                )
            )
            
            # Insert PatchPolynomialPrecondition here
            # Only if enabled and not learnable (learnable is handled in model forward)
            static_precond_enabled = self.hparams.enable_preconditioning and not self.hparams.learnable_preconditioning
            
            transform += PatchPolynomialPrecondition(
                polynomial_type=self.hparams.precondition_type,
                degree=self.hparams.precondition_degree,
                target_field="target",
                enabled=static_precond_enabled,
                store_original=False,
            )
            
            # Continue with remaining transforms
            transform += (
                AddVariateIndex(
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    variate_id_field="variate_id",
                    expected_ndim=3,
                    max_dim=self.hparams.max_dim,
                    randomize=True,
                    collection_type=dict,
                )
                + AddSampleIndex(
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    sample_id_field="sample_id",
                    expected_ndim=3,
                    collection_type=dict,
                )
                + AddTimeIndex(
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    time_id_field="time_id",
                    expected_ndim=3,
                    collection_type=dict,
                )
                + MaskedPrediction(
                    min_mask_ratio=self.hparams.min_mask_ratio,
                    max_mask_ratio=self.hparams.max_mask_ratio,
                    target_field="target",
                    truncate_fields=("variate_id", "time_id", "observed_mask"),
                    optional_truncate_fields=("past_feat_dynamic_real",),
                    prediction_mask_field="prediction_mask",
                    expected_ndim=3,
                )
                + ExtendMask(
                    fields=tuple(),
                    optional_fields=("past_feat_dynamic_real",),
                    mask_field="prediction_mask",
                    expected_ndim=3,
                )
                + FlatPackCollection(
                    field="variate_id",
                    feat=False,
                )
                + FlatPackCollection(
                    field="sample_id",
                    feat=False,
                )
                + FlatPackCollection(
                    field="time_id",
                    feat=False,
                )
                + FlatPackCollection(
                    field="prediction_mask",
                    feat=False,
                )
                + FlatPackCollection(
                    field="observed_mask",
                    feat=True,
                )
                + FlatPackFields(
                    output_field="target",
                    fields=("target",),
                    optional_fields=("past_feat_dynamic_real",),
                    feat=True,
                )
                + SequencifyField(field="patch_size", target_field="target")
                + SelectFields(fields=list(self.seq_fields))
            )

            return transform

        return defaultdict(lambda: default_train_transform)
