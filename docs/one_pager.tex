\documentclass[11pt]{article}
\usepackage[margin=0.85in,top=0.7in,bottom=0.7in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{times}

\setlength{\parskip}{0.4em}
\setlength{\parindent}{0em}
\pagestyle{empty}

\begin{document}

\begin{center}
{\Large\bfseries Universal Sequence Preconditioning for\\[2pt] Zero-Shot Time Series Forecasting}

\vspace{0.6em}

{\normalsize
Jerry Han\textsuperscript{1}\quad
Elad Hazan\textsuperscript{1}
}

\vspace{0.2em}

{\small
\textsuperscript{1}Department of Computer Science, Princeton University
}

\vspace{0.1em}

{\small February 2026}
\end{center}

\vspace{0.5em}

\textbf{Abstract.}\quad
\emph{Universal sequence preconditioning} (USP)~\citep{marsden2025usp} convolves an input sequence with the coefficients of orthogonal polynomials (Chebyshev, Legendre), which corresponds to preconditioning the hidden transition matrix of the underlying dynamical system.
This yields provably improved regret bounds for sequential prediction that are independent of hidden dimension, and has been validated on online learning and recurrent models.
In this work, we extend USP to \emph{time series foundation models}, large pretrained transformers such as Moirai~\citep{woo2024unified} and TimesFM~\citep{das2024decoder} that perform zero-shot forecasting across diverse domains.

We apply causal FIR polynomial filters to the Moirai2 architecture and evaluate systematically on the GIFT-Eval benchmark~\citep{jain2024gifteval}.
Na\"ive application of USP fails for multi-step forecasting: while preconditioning improves training loss as the theory predicts, the reversal step required at inference (recursively undoing the filter using predicted values) causes errors to compound catastrophically, with higher polynomial degrees producing monotonically worse forecasts.

We propose two modifications that rescue the approach.
First, \textbf{patch-aligned stride}: setting the filter's lag spacing equal to the model's patch size aligns the preconditioning with the transformer's tokenization, dramatically reducing reversal error.
Second, we introduce \textbf{spectral hint channels}, which sidestep reversal entirely by providing the polynomial filter residual as an auxiliary input channel alongside the raw time series.
The model predicts in the original coordinate system, requiring no reversal, while the hint channel supplies explicit spectral information about temporal structure as an inductive bias grounded in USP theory~\citep{marsden2025usp, anil2024spectral}.

Spectral hint channels consistently improve zero-shot forecasting, with the largest gains on high-frequency data and long horizons.
Our best model, with only 11.4M parameters, outperforms both the official Moirai-Small and the substantially larger Moirai-Base (${\sim}$90M parameters).
The advantage diminishes with extended training, suggesting that USP-based hint channels accelerate the learning of temporal structure that the model can eventually acquire unaided, but at significantly greater computational cost.

\vspace{0.6em}
\begingroup
\small
\renewcommand{\section}[2]{}
\begin{thebibliography}{5}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}

\bibitem[Marsden \& Hazan(2025)]{marsden2025usp}
A.~Marsden and E.~Hazan.
Universal sequence preconditioning.
\emph{NeurIPS}, 2025.

\bibitem[Woo et~al.(2024)]{woo2024unified}
G.~Woo, C.~Liu, A.~Kumar, C.~Xiong, S.~Savarese, and D.~Sahoo.
Unified training of universal time series forecasting transformers.
\emph{ICML}, 2024.

\bibitem[Das et~al.(2024)]{das2024decoder}
A.~Das, W.~Kong, et~al.
A decoder-only foundation model for time-series forecasting.
\emph{ICML}, 2024.

\bibitem[Jain et~al.(2024)]{jain2024gifteval}
K.~Jain, L.~Bauer, D.~Salinas, M.~Bohlke-Schneider, A.~Alexandrov, and L.~Callot.
{GIFT-Eval}: A benchmark for general time series forecasting model evaluation.
\emph{arXiv:2410.10393}, 2024.

\bibitem[Anil et~al.(2024)]{anil2024spectral}
C.~Anil, E.~Hazan, et~al.
Spectral state space models.
\emph{arXiv:2312.06837}, 2024.

\end{thebibliography}
\endgroup

\end{document}
